%pip install -r ./requirements.txt  
%restart_python  

openpyxl
beautifulsoup4
pycountry
lxml
azure-communication-email
Jinja2
requests
pandas
numpy
bs4
python-dateutil



import os
import re
import time
from datetime import datetime, timedelta
import requests
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup
from dateutil import parser
import pycountry
import openpyxl
import lxml
from io import StringIO
from azure.communication.email import EmailClient

class MaerskScraper:
    def __init__(self):
        self.notices = []
        self.filename = "maersk_compliance_data_master.xlsx"
        self.daily_run = True

    def get_all_notice_urls(self):
        base_url = "https://api.maersk.com/news/articles"
        headers = {
            "accept": "application/json",
            "consumer-key": "opX5WAZLlVYc2SUS7rtdHPECsfjcsQ22"
        }

        filename = self.filename
        
        if os.path.exists(filename):
            page_range = range(1, 3)  # Only recent pages if master exists
            print("üìÅ Master file found. Scanning only recent pages (1-3)")
        else:
            page_range = range(1, 34)  # All pages back to the begining of 2025 if no master
            print("üìÅ Master file not found. Scanning all pages (1-34)")

        all_urls = []
        for page in page_range:
            params = {
                "carrierCode": "MAEU",
                "languageCode": "en",
                "categories": "rate-announcements",
                "limit": 12,
                "page": page
            }

            try:
                res = requests.get(base_url, params=params, headers=headers)
                res.raise_for_status()  # Raise exception for bad status codes
                
                articles = res.json().get("data", {}).get("articles", [])
                if not articles:
                    print(f"‚èπÔ∏è No more articles found on page {page}. Stopping.")
                    break
                    
                for article in articles:
                    path = article.get("articleUrl")
                    if path:
                        full_url = "https://www.maersk.com" + path
                        all_urls.append(full_url)
                        
                #print(f"‚úÖ Processed page {page}, found {len(articles)} articles")
                
            except requests.exceptions.RequestException as e:
                print(f"‚ö†Ô∏è Failed on page {page}: {e}")
                break
            except ValueError as e:
                print(f"‚ö†Ô∏è JSON parsing error on page {page}: {e}")
                break

        print(f"üìä Total URLs collected: {len(all_urls)}")
        return all_urls
    
    def extract_maersk_notice_details(self, url):
        def _get_notice_date_from_url(url):
            match = re.search(r'/news/articles/(\d{4})/(\d{2})/(\d{2})/', url)
            if match:
                year, month, day = match.groups()
                date_obj = datetime.strptime(f"{year}-{month}-{day}", "%Y-%m-%d")
                return date_obj.strftime("%d %b %Y")
            return ""

        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, headers=headers, verify=True)
        soup = BeautifulSoup(res.text, "html.parser")

        # üìù Extract title
        title_text = soup.find("h1").get_text(strip=True) if soup.find("h1") else ""

        # üìù Extract notice content from structured div
        content_div = soup.find("div", class_="rich-text") or soup.find("div", class_="maersk-rich-text")
        full_text = content_div.get_text(separator="\n", strip=True) if content_div else ""
        full_text = full_text.replace('\n', ' ')

        # üìÖ Extract notice date
        notice_date = _get_notice_date_from_url(url)

        # üìä Extract tables
        tables = soup.find_all("table")
        table_dfs = []

        for table in tables:
            try:
                # Wrap the HTML string in StringIO to avoid deprecation warning
                html_string = str(table)
                df = pd.read_html(StringIO(html_string))[0]
                table_dfs.append(df)
            except Exception as e:
                print("Failed to parse table:", e)

        return {
            "Notice Title": title_text,
            "Notice Date": notice_date,
            "Notice Content": full_text,
            "Notice URL": url,
            "Tables": table_dfs
        }

    def extract_effective_date(self, body_text, tables=None):
        def extract_potential_date_strings(text):
             return re.findall(
    r'(?:\b(?:0?\d|1\d|2\d|3[01])\s*(?:st|nd|rd|th)?(?:\s+of)?\s*[-/.\s]*(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|January|February|March|April|May|June|July|August|September|October|November|December)[a-z]*[-/.\s]*(?:\d{2,4})?\b|\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|January|February|March|April|May|June|July|August|September|October|November|December)[a-z]*(?:\s+of)?\s+(?:0?\d|1\d|2\d|3[01])\s*(?:st|nd|rd|th)?,?\s*(?:\d{2,4})?\b|\b(?:0?\d|1\d|2\d|3[01])\s*(?:st|nd|rd|th)?(?:\s+of)?\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|January|February|March|April|May|June|July|August|September|October|November|December)[a-z]*\s*(?:\d{2,4})?\b|\b\d{1,2}\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|January|February|March|April|May|June|July|August|September|October|November|December)[a-z]*\s*(?:\d{4})?\b)',
    text,
    re.IGNORECASE
)

        def parse_date_string(date_str):
            try:
                dt = parser.parse(date_str, dayfirst=True, fuzzy=False, default=datetime(datetime.now().year, 1, 1))
                
                # If year is missing (defaulted to 1900), force current year
                if dt.year == 1900:
                    dt = dt.replace(year=datetime.now().year)

                # If 2-digit year, fix it
                if dt.year < 100:
                    current_century = datetime.now().year // 100 * 100
                    dt = dt.replace(year=current_century + dt.year)
                
                return dt
            except:
                try:
                    dt = parser.parse(date_str, fuzzy=True, default=datetime(datetime.now().year, 1, 1))
                    if dt.year == 1900:
                        dt = dt.replace(year=datetime.now().year)
                    if dt.year < 100:
                        current_century = datetime.now().year // 100 * 100
                        dt = dt.replace(year=current_century + dt.year)
                    return dt
                except:
                    return None


        full_text = body_text
        if tables:
            for table in tables:
                full_text += "\n" + table.to_string(index=False)

        # Normalize text
        clean_text = re.sub(r'\(.*?\)', '', full_text)  # Remove parentheticals
        clean_text = re.sub(r'\s+', ' ', clean_text)    # Normalize whitespace

        # Get all potential date strings
        potential_dates = extract_potential_date_strings(clean_text)
        
        # Parse all candidates and filter valid dates
        current_year = datetime.now().year
        valid_dates = []
        for date_str in potential_dates:
            dt = parse_date_string(date_str)
            if dt and dt.year in {current_year, current_year + 1}:
                valid_dates.append(dt)

        if not valid_dates:
            return None

        # Return the most recent date
        return max(valid_dates).strftime("%d %b %Y")
    
    def apply_effective_date(self, df):
        df["Effective Date"] = df.apply(
            lambda row: self.extract_effective_date(row.get("Notice Content", ""), row.get("Tables", [])),
            axis=1
        )
        return df



    def extract_locations(self, notice_dict):
        text = notice_dict.get("Notice Content", "")
        title = notice_dict.get("Notice Title", "")
        found = set()

        for country in pycountry.countries:
            if country.name in text:
                found.add(country.name)

        aliases = {
            "USA": "United States",
            "usa": "United States",
            "u.s.a.": "United States",
            "u.s.": "United States",
            "US ": "United States",
            "US,": "United States",
            "U.S.": "United States",
            "U.S.A.": "United States",
            "united states of america": "United States",
            "UNITED STATES OF AMERICA": "United States",
            "United States of America": "United States"
        }

        exclusion_phrases = [
            "US government",
            "US Shipping Act",
            "US dollar",
            "US Customs"
        ]

        for alias, standard in aliases.items():
            if alias in text:
                if not any(excl in text for excl in exclusion_phrases):
                    found.add(standard)

        return ", ".join(found)
    

    def extract_notice(self, url):
        return self.extract_maersk_notice_details(url)


    def get_all_notices_df(self):
        urls = self.get_all_notice_urls()
        notices = []

        for url in urls:
            notice = self.extract_notice(url)
            notices.append(notice)

        df = pd.DataFrame(notices) if notices else pd.DataFrame()

        if not df.empty:
            df["Affected Locations"] = df.apply(self.extract_locations, axis=1)

        return df

    
    def compute_date_gap(self, df):
        def gap(row):
            try:
                effective = pd.to_datetime(row['Effective Date'], dayfirst=True, errors='coerce')
                notice = pd.to_datetime(row['Notice Date'], dayfirst=True, errors='coerce')
                if pd.isna(effective) or pd.isna(notice):
                    return None
                return int((effective - notice).days + 1)  # Include both endpoints, return an integer
            except:
                return None

        df['Date Gap'] = df.apply(gap, axis=1)
        return df


    def determine_us_region_impact(self, df):
        us_keywords = {
            'united states', 'puerto rico', 'guam', 'american samoa',
            'northern mariana islands', 'us virgin islands', 'hawaii',
            'alaska', 'american territory', 'world'
        }

        def extract_exclusions(text):
            exclusions = set()
            text = text.lower()
            for keyword in us_keywords:
                if f"exclude {keyword}" in text or f"excluded {keyword}" in text:
                    exclusions.add(keyword)
            return exclusions

        def contains_whole_word(text, keywords):
            return any(re.search(rf'\b{re.escape(kw)}\b', str(text), flags=re.IGNORECASE) for kw in keywords)

        def match_us(row):
            content = str(row['Notice Content'])
            location = str(row['Affected Locations'])
            title = str(row['Notice Title'])

            exclusions = extract_exclusions(content)
            keywords = us_keywords - exclusions

            return (contains_whole_word(location, keywords) or
                    contains_whole_word(title, keywords) or
                    contains_whole_word(content, keywords) or
                    'world' in title.lower())

        df['Affect US Region'] = df.apply(match_us, axis=1)
        return df


    def add_trade_lane_info(self, df):
        def extract_trade_lane(row):
            if not row.get('Affect US Region'):
                return None

            title_lower = str(row.get('notice Title', '')).lower()

            if "world" in title_lower:
                return "all coasts"

            coast_patterns = {
                "east coast & gulf": r"(east coast\s*&\s*gulf)",
                "east & gulf coast": r"(east\s*&\s*gulf coast)",
                "east coast": r"\b(east coast)\b(?!.*&\s*gulf)",
                "west coast": r"\b(west coast)\b",
                "gulf coast": r"\b(gulf coast)\b(?!.*east)"
            }

            coasts = []
            for label, pattern in coast_patterns.items():
                if re.search(pattern, title_lower):
                    coasts.append(label)

            return "all coasts" if not coasts else ", ".join(sorted(coasts))

        df["Trade Lane"] = df.apply(extract_trade_lane, axis=1)
        return df
    
    def calculate_rate_difference(self, tables):
        rate_info = []
        new_rates = []

        for table in tables:
            for col in table.columns:
                col_lower = str(col).lower()

                # Handle combined old/new in one column
                if ('new charge' in col_lower or 'new rate' in col_lower) and \
                ('old charge' in col_lower or 'old rate' in col_lower):
                    for val in table[col]:
                        try:
                            nums = re.findall(r'[\d,]+(?:\.\d+)?', str(val))
                            if len(nums) == 2:
                                old, new = map(lambda x: float(x.replace(',', '')), nums)
                                rate_info.append({'old': old, 'new': new, 'diff': round(new - old, 2)})
                        except:
                            continue
                
                # Store columns for later pair processing
                elif 'old charge' in col_lower or 'old rate' in col_lower:
                    table['__old__'] = table[col].astype(str)
                elif 'new charge' in col_lower or 'new rate' in col_lower:
                    table['__new__'] = table[col].astype(str)

            # Handle separate old and new columns
            if '__new__' in table.columns and '__old__' in table.columns:
                for old_val, new_val in zip(table['__old__'], table['__new__']):
                    try:
                        old = float(re.search(r'[\d,]+(?:\.\d+)?', old_val).group().replace(',', ''))
                        new = float(re.search(r'[\d,]+(?:\.\d+)?', new_val).group().replace(',', ''))
                        rate_info.append({'old': old, 'new': new, 'diff': round(new - old, 2)})
                    except:
                        continue

            elif '__new__' in table.columns:
                for val in table['__new__']:
                    try:
                        new = float(re.search(r'[\d,]+(?:\.\d+)?', str(val)).group().replace(',', ''))
                        new_rates.append(new)
                    except:
                        continue

        if rate_info:  # Only return comparison results if we have both old & new rates
            if any(entry['new'] <= entry['old'] for entry in rate_info):
                return "less than 0"
            else:
                return "greater than 0"
        # If only new_rates exist (no comparison possible), return None
        return None
    

    def flag_rate_increase(self, df):

        negative_patterns = [
        'not a new charge', 'no rate increase', 'not increased', 
        'not implementing', 'not introduced', 'not introducing',
         'not a higher rate', 'refresher', 'refresh',  'withholding tax', 'postponed'
    ]
        
        rate_increase_keywords = [
        'increase', 'increased', 'increasing', 'rate rise', 'tariff hike', 'increased cost',
        'new charge', 'new charges', 'new tariffs', 'higher rate', 'higher charges', 'cost recovery charge',
        'implementing', 'implemented', 'implement',
        'introducing', 'introduced', 'introduce',
        'has reviewed', 'gri'  # GRI = General Rate Increase
    ]

        possible_increase_keywords = ['revising', 'revised', 'revise', 'revises']

        def classify_rate_change(row):
            #text = str(text).lower()
            content = str(row['Notice Content']).lower()
            title = str(row['Notice Title']).lower()
            full_text = f"{title} {content}"

            if any(neg in full_text for neg in negative_patterns):
                return "No"
            
            if any(kw in full_text for kw in rate_increase_keywords):
                return "Yes"
            elif any(kw in full_text for kw in possible_increase_keywords):
                return "Maybe"
            else:
                return "No"

        df['Rate Increase'] = df.apply(classify_rate_change, axis=1)
        return df
    
    
    
    def apply_rate_difference_results(self, df):
        def check(row):
            tables = row.get("Tables", [])
            result = self.calculate_rate_difference(tables)
            return result

        df["Rate Difference"] = df.apply(check, axis=1)
        return df


    def apply_flagging(self, df):
        def flag_row(row):
            affects_us = row.get('Affect US Region', False)
            rate_status = str(row.get('Rate Increase', '')).strip().lower()
            rate_diff = str(row.get('Rate Difference', '')).strip().lower()
            gap = row.get('Date Gap')

            # 1. If it doesn't affect the US region ‚Üí Always No Violation
            if not affects_us:
                return "No Violation"

            # 2. If Date Gap is missing or cannot be parsed ‚Üí Review needed
            try:
                gap = float(gap)
            except:
                return "No Violation"

            # 3. If Date Gap is 30 days or more ‚Üí Always No Violation
            if gap >= 30:
                return "No Violation"

            # 4. If Date Gap < 30 ‚Üí Evaluate based on Rate Increase status
            if gap < 30:
                if rate_status == "no":
                    return "No Violation"
                elif rate_status == "yes":
                    return "Violation"
                
                elif rate_status == "maybe":
                    if pd.isna(row.get("Rate Difference")):
                        return "No Violation"
                    elif "greater than 0" in rate_diff:
                        return "Violation"
                    elif "less than 0" in rate_diff:
                        return "No Violation"
                    else:
                        return "No Violation"

            # Default fallback
            return "No Violation"

        df["Flag"] = df.apply(flag_row, axis=1)
        return df


    def final_formatting(self, df):
        df['Carrier Name'] = 'Maersk'
        df['Notice Date'] = pd.to_datetime(df['Notice Date'], dayfirst=True, errors='coerce').dt.strftime("%d %b %Y")
        df['Effective Date'] = pd.to_datetime(df['Effective Date'], dayfirst=True, errors='coerce').dt.strftime("%d %b %Y")
        df['Trade Lane'] = df['Trade Lane'].str.title()

        final_columns = [
            'Carrier Name', 'Notice Title', 'Notice Date', 'Effective Date',
            'Date Gap', 'Trade Lane', 'Affect US Region', 'Rate Increase', 'Flag', 'Notice URL'  
        ]

        return df[final_columns]


    #-----------------save the processed dataframe to Excel-----------------
    def save_results(self, df):
        filename = self.filename

        try:
            # Read existing master file
            existing_df = pd.read_excel(filename)
            
            # Combine new data with existing data
            combined = pd.concat([df, existing_df], ignore_index=True)
            
            # Convert dates to datetime for proper comparison
            combined["Notice Date"] = pd.to_datetime(combined["Notice Date"], errors="coerce")
            combined["Effective Date"] = pd.to_datetime(combined["Effective Date"], errors="coerce")
            
            # Drop duplicates, keeping the latest notice based on Notice Date
            combined = combined.sort_values("Notice Date", ascending=False)
            combined = combined.drop_duplicates(subset="Notice URL", keep="first")
            
            # Count how many new URLs were added
            new_urls = set(df["Notice URL"]) - set(existing_df["Notice URL"])
            added_count = len(new_urls)

        except FileNotFoundError:
            # If no master file exists, use the current df
            combined = df
            combined["Notice Date"] = pd.to_datetime(combined["Notice Date"], errors="coerce")
            combined["Effective Date"] = pd.to_datetime(combined["Effective Date"], errors="coerce")
            added_count = len(df)

        # Keep only records with Notice Date year >= 2025
        combined = combined[combined["Notice Date"].dt.year >= 2025].copy()

        # Sort newest ‚Üí oldest
        combined.sort_values("Notice Date", ascending=False, inplace=True)

        # Convert to date only (no time)
        combined["Notice Date"] = combined["Notice Date"].dt.date
        combined["Effective Date"] = combined["Effective Date"].dt.date
        
        # Save to Excel
        combined.to_excel(filename, index=False)

        print(f"Added {added_count} new notices. Total in master file: {len(combined)}")


if __name__ == "__main__":
    scraper = MaerskScraper()
    scraper.daily_run = os.path.exists(scraper.filename)

    # Step 1: Load notices
    df = scraper.get_all_notices_df()
    df = scraper.apply_effective_date(df)   # ‚Üê make sure this happens before final_formatting()

    # Optional: Keep a copy of original column names
    original_columns = df.columns.tolist()

    # Step 2: Apply transformations
    df = scraper.compute_date_gap(df)
    df = scraper.determine_us_region_impact(df)
    df = scraper.add_trade_lane_info(df)
    df = scraper.flag_rate_increase(df)
    df = scraper.apply_rate_difference_results(df)
    df = scraper.apply_flagging(df)  
    df = scraper.final_formatting(df)

    scraper.save_results(df)
